# -*- coding: utf-8 -*-
"""Cat Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-NABuBJOITVlW03VvhsQivPNzmNp4yp
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator

#
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn.metrics import confusion_matrix  #Required input to plot_confusion_matrix

#
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

#
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.client import device_lib

#
import pandas as pd
import seaborn as sns
import random
import os

class PredictRNN():

    def __init__(self):

        pass

    def setup(self):

        #print ("DEVICE: ", device_lib.list_local_devices())

        #jobid = os.getenv('SLURM_ARRAY_TASK_ID')
        #name='IA1'

        #self.data_leverpress = np.load('/sfa18k/shreya.saxena/zhangyongxu/Cat/3.7/IA1_data/'+str(name)+'_'+str(jobid)+'.npy',allow_pickle=True) # motor
        #self.data_random = np.load('/sfa18k/shreya.saxena/zhangyongxu/Cat/3.7/IA1_data/'+str(name)+'_random_'+str(jobid)+'.npy',allow_pickle=True) # rest
        self.data_leverpress = np.load(self.fname,allow_pickle=True) # motor
        self.data_random = np.load(self.fname_random,allow_pickle=True) # rest

        # check if data has right size
        print (" [n_trials, n_areas, n_time_steps] ", self.data_leverpress.shape)

    def make_training_data(self):

        # stack all data
        X=np.concatenate((self.data_leverpress,
                          self.data_random),
                          axis=0)
        X=X.transpose((0,2,1))

        # make labels
        self.y=np.concatenate((np.ones(self.data_leverpress.shape[0]),
                          np.zeros(self.data_random.shape[0])))

        # flatten data to 2 columns
        X_R= X.reshape(-1,X.shape[1]*X.shape[2])

        # normalize
        normal_X = preprocessing.normalize(X_R)

        # reconstruct data as per original all, random data order
        X=normal_X.reshape(X.shape[0],
                             X.shape[1],
                             X.shape[2])

        # remove nans
        X_nonnan=X[~np.isnan(X)]
        X=X_nonnan.reshape((X.shape[0],X.shape[1],-1))

        #
        self.X = X


    def model_run(self, lr, epochs, X_train_R, y_train_R, X_test_R, y_test_R,verbose):



        #
        n_timesteps, n_features, n_outputs = X_train_R.shape[1], X_train_R.shape[2], 1 #y_train.shape[1]

        # setup model
        model = tf.keras.models.Sequential([
                                          tf.keras.layers.SimpleRNN(64,
                                                                    input_shape=(n_timesteps,n_features),
                                                                    activation='tanh'),
                                          tf.keras.layers.Dense(n_outputs,activation='sigmoid')
                                        ])
        #
        opt = tf.keras.optimizers.Adam(lr=lr)

        #
        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

        #
        history=model.fit(X_train_R, y_train_R, epochs=epochs,
                      validation_data=(X_test_R, y_test_R),batch_size=5,verbose=verbose)

        #
        acc=history.history['val_accuracy']
        return history, acc

    def run(self):

        # setup object for xvalidation
        kf = KFold(n_splits=10,
                   random_state=None,
                   shuffle=True)

        #
        b_rnn=[]
        for i in range(0,571,30):
          kt_X=self.X[:,i:i+30,:]  # this is a chunk window data grabber;
          acc_rnn_s=np.zeros(10)
          s=0
          for train_index, test_index in kf.split(kt_X):
              print("Train:", train_index, "Validation:",test_index)
              X_train_k, X_test_k = kt_X[train_index], kt_X[test_index]
              y_train_k, y_test_k = self.y[train_index], self.y[test_index]
              #print(" y_train_kF: ", y_train_kF)

              history, acc = self.model_run(0.0001,
                                         200,
                                         X_train_k,
                                         y_train_k,
                                         X_test_k,
                                         y_test_k,
                                         0)
              acc_rnn_s[s]=acc[-1]
              s=s+1

          b_rnn.append(acc_rnn_s)

        b_rnn=np.array(b_rnn)
        a_s=np.mean(b_rnn,axis=1)
        c_s=np.std(b_rnn,axis=1)/(10**0.5)

        np.savez('/home/cat/acc_rnn.npy',
                 b_rnn = b_rnn,
                 a_s = a_s,
                 c_s = c_s)
