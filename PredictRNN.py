# -*- coding: utf-8 -*-
"""Cat Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-NABuBJOITVlW03VvhsQivPNzmNp4yp
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator

#
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn.metrics import confusion_matrix  #Required input to plot_confusion_matrix

#
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

from tqdm import trange
#
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.client import device_lib

#
import pandas as pd
import seaborn as sns
import random
import os

class PredictRNN():

    def __init__(self):

        self.code = 'code_04'

        pass

    def setup(self):

        #print ("DEVICE: ", device_lib.list_local_devices())

        #jobid = os.getenv('SLURM_ARRAY_TASK_ID')
        #name='IA1'

        #self.data_leverpress = np.load('/sfa18k/shreya.saxena/zhangyongxu/Cat/3.7/IA1_data/'+str(name)+'_'+str(jobid)+'.npy',allow_pickle=True) # motor
        #self.data_random = np.load('/sfa18k/shreya.saxena/zhangyongxu/Cat/3.7/IA1_data/'+str(name)+'_random_'+str(jobid)+'.npy',allow_pickle=True) # rest
        self.data_leverpress = np.load(self.fname,allow_pickle=True) # motor
        self.data_random = np.load(self.fname_random,allow_pickle=True) # rest

        # check if data has right size
        print (" [n_trials, n_areas, n_time_steps] ", self.data_leverpress.shape)

    def make_training_data(self):

        # stack all data
        X=np.concatenate((self.data_leverpress,
                          self.data_random),
                          axis=0)
        X=X.transpose((0,2,1))

        # make labels
        self.y=np.concatenate((np.ones(self.data_leverpress.shape[0]),
                          np.zeros(self.data_random.shape[0])))

        # flatten data to 2 columns
        X_R= X.reshape(-1,X.shape[1]*X.shape[2])

        # normalize
        normal_X = preprocessing.normalize(X_R)

        # reconstruct data as per original all, random data order
        X=normal_X.reshape(X.shape[0],
                             X.shape[1],
                             X.shape[2])

        # remove nans
        X_nonnan=X[~np.isnan(X)]
        X=X_nonnan.reshape((X.shape[0],X.shape[1],-1))

        #
        self.X = X


    def model_run(self, lr, epochs, X_train_R, y_train_R, X_test_R, y_test_R, verbose):

        #
        n_timesteps, n_features, n_outputs = X_train_R.shape[1], X_train_R.shape[2], 1 #y_train.shape[1]

        # setup model
        model = tf.keras.models.Sequential([
                                          tf.keras.layers.SimpleRNN(64,
                                                                    input_shape=(n_timesteps,n_features),
                                                                    activation='tanh'),
                                          tf.keras.layers.Dense(n_outputs,activation='sigmoid')
                                        ])
        #
        opt = tf.keras.optimizers.Adam(lr=self.learning_rate)

        #
        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

        #
        history=model.fit(X_train_R,
                          y_train_R,
                          epochs=self.epochs,
                          validation_data=(X_test_R, y_test_R),
                          batch_size=1000,
                          verbose=verbose)

        #
        acc=history.history['val_accuracy']
        return history, acc



    def get_fname(self): # load ordered sessions from file

        self.sessions = np.load(os.path.join(self.main_dir, self.animal_id,'tif_files.npy'))

        data = []
        for k in range(len(self.sessions)):
            data.append(os.path.split(self.sessions[k])[1][:-4])
        self.sessions = data

        #
        final_session = []
        for k in range(len(self.sessions)):
            if str(self.session_id) in str(self.sessions[k]):

                final_session = self.sessions[k]
                break

        self.session = final_session

        print ("self session: ", self.session)

        # select data with or without lockout
        prefix1 = ''
        if self.lockout:
            prefix1 = '_lockout_'+str(self.lockout_window)+"sec"

        # select data with pca compression
        prefix2 = ''
        if self.pca_flag:
            prefix2 = '_pca_'+str(self.pca_var)

        # make fname out for animal + session
        fname = os.path.join(self.main_dir, self.animal_id,
                             'SVM_Scores',
                             'SVM_Scores_'+
                             self.session+"_"+
                             self.code+
                             prefix1+
                             '_trial_ROItimeCourses_'+
                             str(self.window)+'sec'+
                             prefix2+
                             '.npy'
                             )
        self.fname = fname


    def run(self):

        # setup object for xvalidation
        kf = KFold(n_splits=10,
                   random_state=None,
                   shuffle=True)


        #
        self.learning_rate = 0.0001

        #
        self.epochs = 200


        #
        b_rnn=[]
        for i in trange(0,571,30):
          kt_X=self.X[:,i:i+30,:]  # this is a chunk window data grabber;
          acc_rnn_s=np.zeros(10)
          s=0
          for train_index, test_index in kf.split(kt_X):
              #print("Train:", train_index, "Validation:",test_index)
              X_train_k, X_test_k = kt_X[train_index], kt_X[test_index]
              y_train_k, y_test_k = self.y[train_index], self.y[test_index]
              #print(" y_train_kF: ", y_train_kF)

              history, acc = self.model_run(
                                         X_train_k,
                                         y_train_k,
                                         X_test_k,
                                         y_test_k,
                                         0)
              acc_rnn_s[s]=acc[-1]
              s=s+1

          b_rnn.append(acc_rnn_s)

        b_rnn=np.array(b_rnn)
        a_s=np.mean(b_rnn,axis=1)
        c_s=np.std(b_rnn,axis=1)/(10**0.5)

        fname_out = os.path.join(self.main_dir, self.animal_id,'RNN_scores',
                                 self.session_id+str(self.epochs)+"_"+str(self.learning_rate)+'.npy')

        np.savez(fname_out,
                 b_rnn = b_rnn,
                 a_s = a_s,
                 c_s = c_s)
