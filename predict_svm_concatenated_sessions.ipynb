{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import sklearn\n",
    "\n",
    "import parmap\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/media/cat/4TBSSD/yuki/'\n",
    "data_dir = '/media/cat/4TBSSD/yuki/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "def expand(r1, r2):\n",
    "    r1 = torch.from_numpy(r1.transpose(0,2,1))\n",
    "    r2 = torch.from_numpy(r2)\n",
    "    r3 = torch.matmul(r1, r2)\n",
    "    \n",
    "    del r1\n",
    "    del r2\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return r3\n",
    "\n",
    "def load_trial(root_dir, session, subsample,\n",
    "              pca_val):\n",
    "    r1 = np.load(root_dir+session+\"/\"+session+\"_code_04_random_ROItimeCourses_30sec_pca_\"+\n",
    "                 str(pca_val)+\".npy\")[:,:,:900]\n",
    "    r2 = np.load(root_dir+session+\"/\"+session+\"_code_04_random_ROItimeCourses_30sec_pca_\"+\n",
    "                 str(pca_val)+\"_spatial.npy\")\n",
    "\n",
    "    if subsample:\n",
    "        idx = np.random.choice(np.arange(r1.shape[0]), 10,replace=False)\n",
    "        r1 = r1[idx]\n",
    "        \n",
    "    r3 = expand(r1,r2)   \n",
    "    print (r1.shape, \" to \", r3.shape)\n",
    "    \n",
    "    t1 = np.load(root_dir+session+\"/\"+session+\"_code_04_trial_ROItimeCourses_30sec_pca_\"+\n",
    "                 str(pca_val)+\".npy\")[:,:,:900]\n",
    "    t2 = np.load(root_dir+session+\"/\"+session+\"_code_04_trial_ROItimeCourses_30sec_pca_\"+\n",
    "                 str(pca_val)+\"_spatial.npy\")\n",
    "    \n",
    "    # \n",
    "    if subsample:\n",
    "        t1 = t1[idx]\n",
    "    t3 = expand(t1,t2)\n",
    "    \n",
    "    return r3, t3\n",
    "\n",
    "\n",
    "def make_pca_object(r):\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    print (\" data in pre reshape:\", r.shape)\n",
    "    X = r.reshape(r.shape[0]*r.shape[1],\n",
    "                                 r.shape[2])\n",
    "    print (\" data into pca: \", X.shape)\n",
    "\n",
    "    # subselect data\n",
    "    n_selected = min(2500, X.shape[0])\n",
    "    idx = np.random.choice(np.arange(X.shape[0]),n_selected,\n",
    "                           replace=False)\n",
    "    X_select = X[idx]\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(X_select)\n",
    "\n",
    "    import pickle as pk\n",
    "    fname_pca = '/home/cat/pca.pkl'\n",
    "    pk.dump(pca, open(fname_pca,\"wb\"))\n",
    "\n",
    "\n",
    "    # compute # of components needed for reconsturction to the requierd limit\n",
    "    expl_variance = pca.explained_variance_\n",
    "    print (expl_variance)\n",
    "    expl_variance = expl_variance/expl_variance.sum(0)\n",
    "    print (expl_variance)\n",
    "    sums = 0\n",
    "    pca_explained_var_val = 0.99\n",
    "    for k in range(expl_variance.shape[0]):\n",
    "        sums+=expl_variance[k]\n",
    "        if sums>=pca_explained_var_val:\n",
    "            nComp = k+1\n",
    "            break\n",
    "        \n",
    "    #\n",
    "    print (\"nComp required: for var: \", pca_explained_var_val, \" is: \",  k)\n",
    "\n",
    "    nComp = k\n",
    "    print (\"May wish to add at least another few components\")\n",
    "    \n",
    "    return pca, nComp\n",
    "\n",
    "\n",
    "def run_svm_single_randomized_kFold(\n",
    "                                   run_id,\n",
    "                                   idx_trials_split, \n",
    "                                   idx_random_split,\n",
    "                                   window, \n",
    "                                   method, \n",
    "                                   trials, \n",
    "                                   random):\n",
    "\n",
    "    # train data excludes the run_id\n",
    "    idx_trials = np.delete(np.arange(trials.shape[0]),\n",
    "                               idx_trials_split[run_id])\n",
    "    idx_random = np.delete(np.arange(random.shape[0]),\n",
    "                               idx_random_split[run_id])\n",
    "    \n",
    "    # test data is the left over labels\n",
    "    idx_trials_not = np.delete(np.arange(trials.shape[0]),idx_trials)\n",
    "    idx_random_not = np.delete(np.arange(random.shape[0]),idx_random)\n",
    "    \n",
    "    # stack train data\n",
    "    train = np.vstack((trials[idx_trials],random[idx_random]))\n",
    "    labels_train = np.hstack((np.ones(trials[idx_trials].shape[0]),\n",
    "                              np.zeros(random[idx_random].shape[0])))\n",
    "\n",
    "    # stack test data\n",
    "    test = np.vstack((trials[idx_trials_not], random[idx_random_not]))\n",
    "    labels_test = np.hstack((np.ones(trials[idx_trials_not].shape[0]),\n",
    "                             np.zeros(random[idx_random_not].shape[0])))\n",
    "\n",
    "    # \n",
    "    accuracy2=[]\n",
    "    labels2 = []\n",
    "    pred2 = []\n",
    "    for k in range(0, trials.shape[2]-window, 1):\n",
    "        X = train#[:,:,:window]\n",
    "        X = X[:,:,k:k+window]\n",
    "        #if mean_filter:\n",
    "        #    X = np.mean(X,2)\n",
    "\n",
    "        X = X.reshape(train.shape[0],-1)\n",
    "\n",
    "        # \n",
    "        y = labels_train\n",
    "        \n",
    "        #\n",
    "        X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "        # \n",
    "        clf = svm.SVC(kernel=method)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "\n",
    "        # test \n",
    "        X_test = test[:,:,k:k+window]\n",
    "\n",
    "        X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "        X_test = sklearn.preprocessing.scale(X_test)\n",
    "        # \n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # \n",
    "        acc = accuracy_score(labels_test, y_pred)\n",
    "        accuracy2.append(acc)\n",
    "        labels2.append(labels_test)\n",
    "        pred2.append(y_pred)\n",
    "\n",
    "    accuracy2 = np.array(accuracy2)\n",
    "    labels2 = np.array(labels2)\n",
    "    pred2 = np.array(pred2)\n",
    "\n",
    "    #print (\"inner loop: accraucy: \", accuracy2.shape, labels2.shape, pred2.shape)\n",
    "    return accuracy2, labels2, pred2\n",
    "    \n",
    "\n",
    "# \n",
    "def compute_accuracy_svm(trials, \n",
    "                         random, \n",
    "                         data_split, \n",
    "                         window,\n",
    "                         method,\n",
    "                         xvalidation):\n",
    "\n",
    "    \n",
    "    # randomize seed\n",
    "    np.random.seed()\n",
    "\n",
    "    # select groups for parallel processing\n",
    "    run_ids = np.arange(xvalidation)\n",
    "        \n",
    "    idx_trials_split = np.array_split(np.random.choice(np.arange(trials.shape[0]),\n",
    "                                                       trials.shape[0],\n",
    "                                                       replace=False),\n",
    "                                     xvalidation)\n",
    "    \n",
    "    idx_random_split = np.array_split(np.random.choice(np.arange(random.shape[0]),\n",
    "                                                       random.shape[0],\n",
    "                                                       replace=False),\n",
    "                                     xvalidation)\n",
    "    \n",
    "    data = parmap.map(run_svm_single_randomized_kFold,\n",
    "                                       run_ids,\n",
    "                                       idx_trials_split, \n",
    "                                       idx_random_split,\n",
    "                                       window, \n",
    "                                       method, \n",
    "                                       trials, \n",
    "                                       random,\n",
    "                                       pm_processes = 10,\n",
    "                                       pm_pbar=True)\n",
    "    \n",
    "    # \n",
    "    accuracy = []\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for k in range(len(data)):\n",
    "        accuracy.append(data[k][0].T)\n",
    "        labels.append(data[k][1].T)\n",
    "        predictions.append(data[k][2].T)\n",
    "      \n",
    "    accuracy = np.vstack(accuracy).T\n",
    "    labels = np.vstack(labels).T\n",
    "    predictions = np.vstack(predictions).T\n",
    "    \n",
    "    \n",
    "    return accuracy, labels, predictions\n",
    "\n",
    "def pca_denoise_data(pca, data_stm, nComp):\n",
    "    \n",
    "    # \n",
    "    X = data_stm.reshape(data_stm.shape[0]*data_stm.shape[1],\n",
    "                         data_stm.shape[2])\n",
    "\n",
    "    #\n",
    "    time_filters = pca.transform(X)[:,:nComp]\n",
    "    pca_time_filters = time_filters.reshape(data_stm.shape[0],\n",
    "                                                 data_stm.shape[1],\n",
    "                                                 -1).transpose(0,2,1)\n",
    "    pca_spatial_filters = pca.components_[:nComp,:]\n",
    "\n",
    "\n",
    "    print (\"... made data: \", pca_time_filters.shape)\n",
    "    return pca_time_filters, pca_spatial_filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 900)  to  torch.Size([10, 900, 16384])\n",
      "done session:  IJ2pm_Feb29_30Hz\n",
      "(10, 6, 900)  to  torch.Size([10, 900, 16384])\n",
      "done session:  IJ2pm_Mar1_30Hz\n",
      "(10, 6, 900)  to  torch.Size([10, 900, 16384])\n",
      "done session:  IJ2pm_Mar2_30Hz\n",
      "(10, 6, 900)  to  torch.Size([10, 900, 16384])\n",
      "done session:  IJ2pm_Mar3_30Hz\n",
      "(10, 6, 900)  to  torch.Size([10, 900, 16384])\n",
      "done session:  IJ2am_Mar7_30Hz\n",
      "Total stack:  (100, 900, 16384)\n",
      " data in pre reshape: (100, 900, 16384)\n",
      " data into pca:  (90000, 16384)\n",
      "[7.15567277e+00 2.07792531e+00 3.03738132e-01 ... 5.92807726e-32\n",
      " 1.66282450e-32 1.36525688e-32]\n",
      "[7.15965431e-01 2.07908151e-01 3.03907137e-02 ... 5.93137574e-33\n",
      " 1.66374973e-33 1.36601653e-33]\n",
      "nComp required: for var:  0.99  is:  5\n",
      "May wish to add at least another few components\n",
      "manually set nComp to  20\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "\n",
    "# STEP 1: Grab subsamples of sessoin data\n",
    "animal_id = 'IJ2'\n",
    "\n",
    "#sessions = 'all'\n",
    "sessions = ['IJ2pm_Feb29_30Hz','IJ2pm_Mar1_30Hz','IJ2pm_Mar2_30Hz','IJ2pm_Mar3_30Hz','IJ2am_Mar7_30Hz' ]\n",
    "\n",
    "\n",
    "#\n",
    "pca_val = '0.95'   # load only 0.95 pca for individual sessions but increase pca_val to ncomp = 20 for conncateated pca\n",
    "\n",
    "# \n",
    "r = np.zeros((0,900,16384),'float32')\n",
    "subsample= True #take only 10 trials per session\n",
    "for session in sessions:\n",
    "    root_dir = '/media/cat/4TBSSD/yuki/IJ2/tif_files/'\n",
    "    \n",
    "    # \n",
    "    a, b = load_trial(root_dir, session, subsample, pca_val)\n",
    "    r = np.vstack((r,a))\n",
    "    r = np.vstack((r,b))\n",
    "    print (\"done session: \", session)\n",
    "\n",
    "print (\"Total stack: \", r.shape)\n",
    "\n",
    "# STEP 2: Get PCA object and get nComp required to reconstruct \n",
    "pca, nComp = make_pca_object(r)\n",
    "\n",
    "#\n",
    "nComp = 20\n",
    "print (\"manually set nComp to \", nComp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 7, 900)  to  torch.Size([89, 900, 16384])\n",
      "(58, 6, 900)  to  torch.Size([58, 900, 16384])\n",
      "(100, 6, 900)  to  torch.Size([100, 900, 16384])\n",
      "(45, 6, 900)  to  torch.Size([45, 900, 16384])\n",
      "(31, 6, 900)  to  torch.Size([31, 900, 16384])\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "###### PCA DENOISING OF INDIVIDUAL SESSIONS ####\n",
    "################################################\n",
    "# \n",
    "sessions = ['IJ2pm_Feb29_30Hz','IJ2pm_Mar1_30Hz','IJ2pm_Mar2_30Hz','IJ2pm_Mar3_30Hz','IJ2am_Mar7_30Hz' ]\n",
    "save_dir = '/media/cat/4TBSSD/yuki/IJ2/concatenation_tests/'\n",
    "\n",
    "# \n",
    "subsample=False\n",
    "r = np.zeros((0,900,16384),'float32')\n",
    "t = np.zeros((0,900,16384),'float32')\n",
    "\n",
    "# \n",
    "nComp = 20\n",
    "for session in sessions:\n",
    "    root_dir = '/media/cat/4TBSSD/yuki/IJ2/tif_files/'\n",
    "    a, b = load_trial(root_dir, session, subsample, pca_val)\n",
    "    \n",
    "    t1, t2 = pca_denoise_data(pca, a, nComp)\n",
    "    np.save(save_dir + session+\"_trial.npy\", t1)\n",
    "    np.save(save_dir + session+\"_trial_space.npy\", t2)\n",
    "    \n",
    "    r1, r2 = pca_denoise_data(pca, b, nComp)\n",
    "    np.save(save_dir + session+\"_random.npy\", r1)\n",
    "    np.save(save_dir + session+\"_random_space.npy\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323, 20, 900) (334, 20, 900)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "###### LOAD PREVIOUSLY GLOBAL COMPRESSED SESSIONS AND CONCATENATE #######\n",
    "#########################################################################\n",
    "save_dir = '/media/cat/4TBSSD/yuki/IJ2/concatenation_tests/'\n",
    "\n",
    "sessions = ['IJ2pm_Feb29_30Hz','IJ2pm_Mar1_30Hz','IJ2pm_Mar2_30Hz','IJ2pm_Mar3_30Hz','IJ2am_Mar7_30Hz' ]\n",
    "\n",
    "trials = []\n",
    "random = []\n",
    "for session in sessions:\n",
    "    trials.append(np.load(save_dir+session+\"_trial.npy\"))\n",
    "    random.append(np.load(save_dir+session+\"_random.npy\"))\n",
    "    \n",
    "trials = np.vstack(trials)\n",
    "random = np.vstack(random)\n",
    "print (trials.shape, random.shape)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials:  (323, 20, 900)  random:  (323, 20, 900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:53<00:00, 11.34s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870, 5)\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "######### COMPARE SINGLE VS. CONCATENATED SVM PREDICTION ##########\n",
    "###################################################################\n",
    "\n",
    "root_dir = '/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/'\n",
    "sessions = ['IJ2pm_Feb29_30Hz','IJ2pm_Mar1_30Hz','IJ2pm_Mar2_30Hz','IJ2pm_Mar3_30Hz','IJ2am_Mar7_30Hz' ]\n",
    "\n",
    "ctr=1\n",
    "n_trials = [90,58,99,43,33]\n",
    "for session in sessions:\n",
    "    ax=plt.subplot(2,3,ctr)\n",
    "    \n",
    "    data = np.load(root_dir + \"SVM_Scores_\"+session+\"code_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz\")\n",
    "\n",
    "    data = data['accuracy']\n",
    "    # \n",
    "    mean = data.mean(1)\n",
    "    std = np.std(data,1)\n",
    "    t = np.arange(mean.shape[0])/30-29\n",
    "    \n",
    "    # \n",
    "    plt.plot(t, mean, c='blue')\n",
    "    plt.fill_between(t, mean+std, mean-std, color='blue', alpha=.2,\n",
    "                label=session)\n",
    "                   \n",
    "    data =np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_concatenatedcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz')\n",
    "    data = data['accuracy']\n",
    "\n",
    "    mean = data.mean(1)\n",
    "    std = np.std(data,1)\n",
    "    t = np.arange(mean.shape[0])/30-29\n",
    "    \n",
    "    # \n",
    "    plt.plot(t, mean, c='black')\n",
    "    plt.fill_between(t, mean+std, mean-std, color='black', alpha=.2,\n",
    "                label='concatenated')\n",
    "    plt.legend()\n",
    "    plt.title(session+ \"  # trials: \" +str(n_trials[ctr-1]))\n",
    "    plt.ylim(0.4, 1.0)\n",
    "    plt.xlim(-29,0)\n",
    "    plt.plot([-29,0],[0.5,0.5],'r--')\n",
    "    ctr+=1\n",
    "print (data.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870, 10)\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/'\n",
    "sessions = ['IJ2pm_Feb29_30Hz','IJ2pm_Mar1_30Hz','IJ2pm_Mar2_30Hz','IJ2pm_Mar3_30Hz','IJ2am_Mar7_30Hz' ]\n",
    "\n",
    "ax=plt.subplot(1,1,1)\n",
    "    \n",
    "data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_concatenatedcode_04_trial_ROItimeCourses_30sec_Xvalid10_Slidewindow30.npz')\n",
    "data = data['accuracy']\n",
    "    # \n",
    "mean = data.mean(1)\n",
    "std = np.std(data,1)\n",
    "t = np.arange(mean.shape[0])/30-29\n",
    "\n",
    "# \n",
    "plt.plot(t, mean, c='blue')\n",
    "plt.fill_between(t, mean+std, mean-std, color='blue', alpha=.2,\n",
    "            label='latest')\n",
    "\n",
    "# # \n",
    "# data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_concatenatedcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30_newmethod.npz')\n",
    "# data = data['accuracy']\n",
    "\n",
    "# # \n",
    "# mean = data.mean(1)\n",
    "# std = np.std(data,1)\n",
    "# t = np.arange(mean.shape[0])/30-29\n",
    "\n",
    "# # \n",
    "# plt.plot(t, mean, c='black')\n",
    "# plt.fill_between(t, mean+std, mean-std, color='black', alpha=.2,\n",
    "#             label='latest_previous')\n",
    "\n",
    "\n",
    "# \n",
    "data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_concatenatedcode_04_trial_ROItimeCourses_30sec_Xvalid10_Slidewindow30_5PCA.npz')\n",
    "data = data['accuracy']\n",
    "\n",
    "# \n",
    "mean = data.mean(1)\n",
    "std = np.std(data,1)\n",
    "t = np.arange(mean.shape[0])/30-29\n",
    "\n",
    "# \n",
    "plt.plot(t, mean, c='red')\n",
    "plt.fill_between(t, mean+std, mean-std, color='red', alpha=.2,\n",
    "            label='previous 5 pca')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "#plt.title(session+ \"  # trials: \" +str(n_trials[ctr-1]))\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.xlim(-29,0)\n",
    "plt.plot([-29,0],[0.5,0.5],'r--')\n",
    "ctr+=1\n",
    "\n",
    "# \n",
    "print (data.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  (1771, 182)\n",
      "pred:  (1771, 182)\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "######### SINGLE TRIAL PREDICTION RESULTS ######\n",
    "################################################\n",
    "\n",
    "data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_IJ2pm_Feb29_30Hzcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz', allow_pickle=True)\n",
    "#data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_IJ2pm_Mar2_30Hzcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz', allow_pickle=True)\n",
    "#data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_IJ2pm_Mar3_30Hzcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz')\n",
    "#data = np.load('/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_concatenatedcode_04_trial_ROItimeCourses_30sec_Xvalid5_Slidewindow30.npz')\n",
    "labels = data['labels']\n",
    "print (\"labels: \", labels.shape)\n",
    "\n",
    "pred = data['predictions']\n",
    "print (\"pred: \", pred.shape)\n",
    "\n",
    "clrs = ['black','blue']\n",
    "trial_selected = np.random.choice(np.arange(labels.shape[1]))\n",
    "\n",
    "# plot piece wise cumulative results\n",
    "ax=plt.subplot(111)\n",
    "t=np.arange(300)/30-10.\n",
    "\n",
    "for trial in range(labels.shape[1]):\n",
    "    plt.plot(t,pred[600:900,trial].cumsum(), \n",
    "         c= clrs[int(labels[0,trial])], alpha=.5)\n",
    "    if trial ==trial_selected:\n",
    "        plt.plot(t,pred[600:900,trial].cumsum(), \n",
    "         c= 'red',\n",
    "                linewidth=5,\n",
    "                label=\"example trial label: \"+str(labels[0,trial_selected]))\n",
    "\n",
    "#\n",
    "plt.plot([t[0],t[-1]],[0,300],'--',c='blue',linewidth=10,\n",
    "         label=\"perfect decode leverpull\", alpha=.5)\n",
    "plt.plot([t[0],t[-1]],[0,0],'--',c='black',linewidth=10,\n",
    "         label=\"perfect decode random\", alpha=.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-55698ca79cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array_true' is not defined"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "########## VISUALIZE SVM FOR EACH TRIAL ###########\n",
    "###################################################\n",
    "clrs = ['black','blue','red','green','cyan','brown','pink','magenta','grey','yellow']\n",
    "fig=plt.figure()\n",
    "ax=plt.subplot(121)\n",
    "plt.plot(array_true.T.sum(1))\n",
    "\n",
    "ax=plt.subplot(122)\n",
    "plt.plot(array_false.T.sum(1))\n",
    "plt.show()\n",
    "plt.suptitle(\"accuracy for each class\")    \n",
    "\n",
    "# \n",
    "fig=plt.figure()\n",
    "\n",
    "ax=plt.subplot(121)\n",
    "for k in range(array_true.shape[0]):\n",
    "    plt.plot(np.cumsum(array_true[k]), c=clrs[k])\n",
    "plt.ylim(0,420)\n",
    "ax=plt.subplot(122)\n",
    "for k in range(array_false.shape[0]):\n",
    "    plt.plot(np.cumsum(array_false[k]), c=clrs[k])\n",
    "#plt.ylim(0,420)\n",
    "plt.suptitle(\"cumulative prediction at each time point for both classes\")    \n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "array1 = array_true.copy()\n",
    "array2 = array_false.copy()\n",
    "\n",
    "idx = np.where(array1==0)\n",
    "array1[idx]=-1\n",
    "idx = np.where(array2==0)\n",
    "array2[idx]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870, 5)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n",
      "..>DONE...\n",
      "(10, 449)\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "###### MODEL RESULTS AS LEAKY INTEGRATORS ######\n",
    "################################################\n",
    "def model(array, ax, params, plotting=False):\n",
    "    \n",
    "    \n",
    "    t=(np.arange(0,array.shape[1],1)-420)/30.\n",
    "    \n",
    "    # \n",
    "    prediction = np.zeros((array.shape),'float32')\n",
    "    for k in range(array.shape[0]):\n",
    "        #cs = np.zeros(array[k].shape[0], 'float32')\n",
    "        for p in range(1,array[k].shape[0],1):\n",
    "\n",
    "            # grab prediction\n",
    "            temp = array[k][p]*params.scaling\n",
    "\n",
    "            # urgency\n",
    "            if params.urgency:\n",
    "                #temp = temp*((k+urgency_scaling)/(urgency_scaling*2))\n",
    "                temp = temp*(p**params.urgency_scaling/(\n",
    "                                array[k].shape[0]**params.urgency_scaling))\n",
    "                #print (k,p,temp)\n",
    "\n",
    "            # cumulative part\n",
    "            temp = temp+prediction[k,p-1]\n",
    "\n",
    "            # noise comp\n",
    "            if noise:\n",
    "                temp = temp + np.random.normal(0, params.noise_sigma)\n",
    "\n",
    "            # leaky component\n",
    "            if leaky:\n",
    "                temp = temp - np.sign(temp)*abs(np.random.normal(0, params.leaky_sigma))\n",
    "\n",
    "            # check if threshold reached\n",
    "            if abs(temp)>=params.threshold:\n",
    "                prediction[k,p:]=temp\n",
    "                break\n",
    "            else:\n",
    "                prediction[k,p]=temp \n",
    "\n",
    "        if plotting:\n",
    "            ax.plot(t,prediction[k], c=clrs[k])\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def compute_accuracy(pred1, pred2):\n",
    "    \n",
    "    print (pred1.shape)\n",
    "    true_pos = np.zeros(pred1.shape[1],'float32')\n",
    "    false_neg = np.zeros(pred1.shape[1],'float32')\n",
    "    for k in range(pred1.shape[1]):\n",
    "        idx = np.where(pred1[:,k]>0)[0]\n",
    "        true_pos[k] = idx.shape[0]\n",
    "        false_neg[k] = (pred1.shape[0]-true_pos[k])\n",
    "            \n",
    "    false_pos = np.zeros(pred2.shape[1],'float32')\n",
    "    true_neg = np.zeros(pred2.shape[1],'float32')\n",
    "    for k in range(pred2.shape[1]):\n",
    "        idx = np.where(pred2[:,k]>0)[0]\n",
    "        false_pos[k] = idx.shape[0]\n",
    "        true_neg[k] = (pred2.shape[0]-false_pos[k])\n",
    "    \n",
    "    # compute sensitivity:  true positives / (true positives + false negatives)\n",
    "    #sens.append(true_pos / (true_pos+false_neg))\n",
    "    #spec.append(true_neg / (true_neg+false_pos))\n",
    "\n",
    "    all_pos = true_pos + false_neg\n",
    "    all_neg = false_pos + true_neg\n",
    "    \n",
    "    # compute accuracy\n",
    "    accuracy = (true_pos+true_neg)/(all_pos+all_neg)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "#  class to hold the parameters\n",
    "class params():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# \n",
    "fig=plt.figure()\n",
    "for k in range(10):\n",
    "    p = params\n",
    "    p.scaling = 0.1\n",
    "    p.noise_sigma = 0.0\n",
    "    p.leaky_sigma = 0.05\n",
    "    p.threshold = 5.0\n",
    "    p.urgency = True\n",
    "    p.urgency_scaling = 1.+k*0.2\n",
    "\n",
    "    #\n",
    "    p.noise = False\n",
    "    p.leaky = True\n",
    "    p.plotting = False\n",
    "\n",
    "    # \n",
    "    ax=[]\n",
    "    if p.plotting:\n",
    "        fig=plt.figure()\n",
    "        ax=plt.subplot(121)\n",
    "    pred1 = model(array1, ax, p)\n",
    "\n",
    "    # \n",
    "    ax2=[]\n",
    "    if p.plotting:\n",
    "        ax2=plt.subplot(122)\n",
    "    pred2 = model(array2, ax2, p)\n",
    "\n",
    "    print (\"..>DONE...\")\n",
    "\n",
    "\n",
    "    accuracy = compute_accuracy(pred1,pred2)\n",
    "    t=(np.arange(0,pred1.shape[1],1)-420)/30.\n",
    "    plt.plot(t, accuracy, c=clrs[k], linewidth=4, alpha=.4)\n",
    "\n",
    "    \n",
    "plt.plot(t,accuracy_svm, c='black',linewidth=4)\n",
    "plt.ylim(0.3,1.0)\n",
    "plt.xlim(-15,0)\n",
    "plt.plot([-15,0],[0.5,0.5])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 420)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PredictSVMTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-48cd169423b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m##### PREDICT SVM DECISION TIME #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictSVMTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/cat/4TBSSD/yuki/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PredictSVMTime' is not defined"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "##### PREDICT SVM DECISION TIME #####\n",
    "#####################################\n",
    "svm = PredictSVMTime()\n",
    "svm.main_dir = '/media/cat/4TBSSD/yuki/'\n",
    "\n",
    "# default params\n",
    "svm.lockout = False\n",
    "svm.lockout_window = 10\n",
    "svm.pca_flag = True\n",
    "svm.pca_var = 0.95\n",
    "svm.window = 10\n",
    "svm.use_saved_model = False\n",
    "svm.fname_saved_model = '/media/cat/4TBSSD/yuki/AQ2/tif_files/AQ2am_Feb11_30Hz/analysis/00599_svm.pkl'\n",
    "\n",
    "# \n",
    "names = ['IA1','IA2','IA3','IJ1','IJ2','AQ2'] # \"AR4\" and other datasets could work\n",
    "\n",
    "# \n",
    "lockouts = [False, True]\n",
    "for lockout in lockouts:\n",
    "    svm.lockout=lockout\n",
    "    for id_ in names:\n",
    "        svm.animal_id = id_\n",
    "        svm.session_id = 'all'\n",
    "\n",
    "        # \n",
    "        svm.predict()\n",
    "\n",
    "# svm.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AQ2am_Dec9_30Hz' 'AQ2am_Dec10_30Hz' 'AQ2pm_Dec10_30Hz'\n",
      " 'AQ2am_Dec11_30Hz' 'AQ2pm_Dec14_30Hz' 'AQ2am_Dec14_30Hz'\n",
      " 'AQ2pm_Dec16_30Hz' 'AQ2am_Dec17_30Hz' 'AQ2pm_Dec17_30Hz'\n",
      " 'AQ2am_Dec18_30Hz' 'AQ2pm_Dec18_30Hz' 'AQ2am_Dec21_30Hz'\n",
      " 'AQ2am_Dec22_30Hz' 'AQ2am_Dec23_30Hz' 'AQ2am_Dec28_30Hz'\n",
      " 'AQ2am_Dec29_30Hz' 'AQ2am_Dec30_30Hz' 'AQ2am_Dec31_30Hz'\n",
      " 'AQ2am_Jan4_30Hz' 'AQ2am_Jan5_30Hz' 'AQ2am_Jan6_30Hz' 'AQ2am_Jan7_30Hz'\n",
      " 'AQ2am_Jan8_30Hz' 'AQ2pm_Jan11_30Hz' 'AQ2am_Jan11_30Hz'\n",
      " 'AQ2pm_Jan12_30Hz' 'AQ2am_Jan12_30Hz' 'AQ2pm_Jan13_30Hz'\n",
      " 'AQ2am_Jan13_30Hz' 'AQ2pm_Jan14_30Hz' 'AQ2am_Jan14_30Hz'\n",
      " 'AQ2am_Jan15_30Hz' 'AQ2pm_Jan15_30Hz' 'AQ2pm_Jan18_30Hz'\n",
      " 'AQ2am_Jan18_30Hz' 'AQ2am_Jan19_30Hz' 'AQ2pm_Jan19_30Hz'\n",
      " 'AQ2pm_Jan20_30Hz' 'AQ2am_Jan20_30Hz' 'AQ2pm_Jan21_30Hz'\n",
      " 'AQ2am_Jan21_30Hz' 'AQ2pm_Jan22_30Hz' 'AQ2am_Jan22_30Hz'\n",
      " 'AQ2am_Jan25_30Hz' 'AQ2pm_Jan25_30Hz' 'AQ2pm_Jan26_30Hz'\n",
      " 'AQ2am_Jan26_30Hz' 'AQ2am_Jan27_30Hz' 'AQ2pm_Jan27_30Hz'\n",
      " 'AQ2am_Jan28_30Hz' 'AQ2pm_Jan28_30Hz' 'AQ2am_Jan29_30Hz'\n",
      " 'AQ2pm_Jan29_30Hz' 'AQ2am_Feb2_30Hz' 'AQ2am_Feb3_30Hz' 'AQ2am_Feb4_30Hz'\n",
      " 'AQ2am_Feb5_30Hz' 'AQ2am_Feb9_30Hz' 'AQ2am_Feb10_30Hz' 'AQ2am_Feb11_30Hz'\n",
      " 'AQ2am_Feb12_30Hz' 'AQ2am_Feb15_30Hz' 'AQ2am_Feb16_30Hz'\n",
      " 'AQ2am_Feb17_30Hz' 'AQ2am_Feb18_30Hz' 'AQ2am_Feb19_30Hz'\n",
      " 'AQ2am_Feb22_30Hz' 'AQ2am_Feb23_30Hz' 'AQ2am_Feb25_30Hz'\n",
      " 'AQ2am_Feb26_30Hz' 'AQ2am_Feb29_30Hz' 'AQ2am_Mar1_30Hz' 'AQ2am_Mar2_30Hz'\n",
      " 'AQ2am_Mar3_30Hz' 'AQ2pm_Mar7_Day3_30Hz' 'AQ2pm_Mar9_Day5_30Hz'\n",
      " 'AQ2pm_Mar11_Day7_30Hz' 'AQ2am_Mar14_Week2_30Hz' 'AQ2pm_Mar15_Week2_30Hz'\n",
      " 'AQ2am_Mar16_Week2_30Hz' 'AQ2am_Mar17_Week2_30Hz'\n",
      " 'AQ2am_Mar18_Week2_30Hz' 'AQ2am_Mar21_Week3_30Hz'\n",
      " 'AQ2am_Mar22_Week3_30Hz' 'AQ2am_Mar23_Week3_30Hz'\n",
      " 'AQ2am_Mar24_Week3_30Hz' 'AQ2am_Mar29_Week4_30Hz'\n",
      " 'AQ2am_Mar30_Week4_30Hz' 'AQ2am_Mar31_Week4_30Hz' 'AQ2am_Apr1_Week4_30Hz'\n",
      " 'AQ2am_Apr4_Week5_30Hz' 'AQ2am_Apr5_Week5_30Hz' 'AQ2am_Apr6_Week5_30Hz'\n",
      " 'AQ2am_Apr7_Week5_30Hz' 'AQ2am_Apr8_Week5_30Hz' 'AQ2am_Apr11_Week6_30Hz'\n",
      " 'AQ2am_Apr12_Week6_30Hz' 'AQ2am_Apr13_Week6_30Hz'\n",
      " 'AQ2am_Apr14_Week6_30Hz' 'AQ2am_Apr15_Week6_30Hz'\n",
      " 'AQ2am_Apr18_Week7_30Hz' 'AQ2am_Apr19_Week7_30Hz'\n",
      " 'AQ2am_Apr20_Week7_30Hz' 'AQ2am_Apr21_Week7_30Hz'\n",
      " 'AQ2am_Apr22_Week7_30Hz' 'AQ2am_Apr25_Week8_30Hz'\n",
      " 'AQ2am_Apr26_Week8_30Hz' 'AQ2am_Apr27_Week8_30Hz'\n",
      " 'AQ2am_Apr28_Week8_30Hz' 'AQ2am_Apr29_Week8_30Hz']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'AQ2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "################################################\n",
    "################################################\n",
    "fig = plt.figure()\n",
    "svm.animal_id = 'AQ2'\n",
    "svm.session_id = 'all'\n",
    "svm.get_sessions()\n",
    "print (svm.sessions)\n",
    "\n",
    "#\n",
    "ctr=1\n",
    "for session_id in svm.sessions:\n",
    "    ax=plt.subplot(8,10,ctr)\n",
    "    svm.session_id = session_id\n",
    "    try:\n",
    "        svm.plot_decision_time(ax)\n",
    "        plt.title(session_id+\", #: \"+str(svm.n_trials), fontsize=6)\n",
    "        if ctr!=1:\n",
    "            plt.yticks([])\n",
    "        if ctr!=31:\n",
    "            plt.xticks([])\n",
    "\n",
    "        ctr+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "plt.suptitle(svm.animal_id,fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['IA1','IA2','IA3','IJ1','AQ2'] # \"AR4\" and other datasets could work\n",
    "\n",
    "for name in names:\n",
    "    rnn.animal_id = name\n",
    "    rnn.session_id = 'all'\n",
    "\n",
    "    rnn.run_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-e7b69b6f7608>:48: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"r--\" (-> color='r'). The keyword argument will take precedence.\n",
      "  plt.plot([-10,0],[0.5,0.5],'r--',c='black')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "data1 = np.load('/media/cat/4TBSSD/yuki/IJ2/RNN_scores/Mar3_200_0.0001.npy.npz')\n",
    "data1 = np.load('/media/cat/4TBSSD/yuki/IJ2/RNN_scores/IJ2pm_Mar2_30Hz_1000_0.0001.npz')\n",
    "#data1 = np.load('/media/cat/4TBSSD/yuki/IJ2/RNN_scores/IJ2pm_Feb29_30Hz_1000_0.0001.npz')\n",
    "b = data1['b_rnn']\n",
    "c = data1['c_s']\n",
    "\n",
    "#\n",
    "print (b.shape)\n",
    "mean = b.mean(1)\n",
    "t=np.linspace(-9.5, 0, mean.shape[0])\n",
    "\n",
    "ax=plt.subplot(111)\n",
    "\n",
    "plt.plot(t, mean, c='black')\n",
    "plt.fill_between(t, mean+c, mean-c, color='black', alpha=.2,\n",
    "                label='RNN - 1000epochs')\n",
    "\n",
    "#\n",
    "data1 = np.load('/media/cat/4TBSSD/yuki/IJ2/RNN_scores/IJ2pm_Mar2_30Hz_200_0.0001.npz')\n",
    "#data1 = np.load('/media/cat/4TBSSD/yuki/IJ2/RNN_scores/IJ2pm_Feb29_30Hz_200_0.0001.npz')\n",
    "b = data1['b_rnn']\n",
    "c = data1['c_s']\n",
    "\n",
    "#\n",
    "print (b.shape)\n",
    "mean = b.mean(1)\n",
    "t=np.linspace(-9.5, 0, mean.shape[0])\n",
    "plt.plot(t, mean, c='red')\n",
    "plt.fill_between(t, mean+c, mean-c, color='red', alpha=.2,\n",
    "                label='RNN - 200epochs')\n",
    "#\n",
    "if False:\n",
    "    fname_svm = '/media/cat/4TBSSD/yuki/IJ2/SVM_Scores/SVM_Scores_IJ2pm_Mar2_30Hz_code_04_trial_ROItimeCourses_10sec_pca_0.95.npy'\n",
    "    data2 = np.load(fname_svm)\n",
    "    print (data2.shape)\n",
    "    data2 = data2[:300]\n",
    "    mean = data2.mean(1)\n",
    "    std = data2.std(1)\n",
    "    t= np.linspace(t[0],t[-1],mean.shape[0])\n",
    "    plt.plot(t, mean, 'blue')\n",
    "    plt.fill_between(t, mean+std, mean-std, color='blue', alpha=.2,\n",
    "                    label='SVM - 2 frames sliding window')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlim(-10,0)\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.plot([-10,0],[0.5,0.5],'r--',c='black')\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "#plt.suptitle(os.path.split(fname_svm)[1])\n",
    "plt.xlabel(\"Time (sec)\",fontsize=20)\n",
    "plt.ylabel(\"Decoding accuracy\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLDER CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self session: IA1pm_Feb23_30Hz\n",
      "Self session: IA1pm_Feb23_30Hz\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "############## VISUALIZE RESULTS ############\n",
    "#############################################\n",
    "    \n",
    "# LEVER PULL\n",
    "vis = Visualize()\n",
    "\n",
    "# lever-related data\n",
    "vis.main_dir = data_dir\n",
    "vis.random_flag = False  # shuffle data to show baseline\n",
    "\n",
    "# \n",
    "vis.window = 10\n",
    "vis.lockout_window = 10\n",
    "vis.code = 'code_04'\n",
    "vis.lockout = False\n",
    "\n",
    "vis.animal_id = \"IA1\"\n",
    "vis.session_id = \"Feb23_\"\n",
    "title = vis.animal_id + \"  \"+vis.session_id\n",
    "\n",
    "#############################################\n",
    "############## DECISION TYPE ################\n",
    "#############################################\n",
    "# select animal and session\n",
    "\n",
    "# load pca \n",
    "fig =plt.figure()\n",
    "\n",
    "# ax=plt.subplot(2,3,1)\n",
    "# vis.pca_flag = False\n",
    "# vis.lockout = True\n",
    "# vis.pca_var = 0.99\n",
    "# vis.plot_decision_choice('black',str(vis.pca_var),title, ax)\n",
    "\n",
    "# vis.lockout = True\n",
    "# vis.plot_decision_choice('blue',str(vis.pca_var)+\" lockout\",title, ax)\n",
    "\n",
    "# \n",
    "# vis.lockout = True\n",
    "# vis.plot_decision_choice('blue',str(vis.pca_var) + \" lockout\",title, ax)\n",
    "\n",
    "\n",
    "# \n",
    "vars_ = [0.95]\n",
    "for k in range(len(vars_)):\n",
    "    #ax=plt.subplot(2,3,k+2)\n",
    "    ax=plt.subplot(1,1,k+1)\n",
    "    vis.pca_flag = True\n",
    "    vis.lockout = False\n",
    "    vis.pca_var = vars_[k]\n",
    "    vis.plot_decision_choice('black',str(vis.pca_var),title, ax)\n",
    "\n",
    "    # \n",
    "    vis.pca_flag = True\n",
    "    vis.lockout = True\n",
    "    vis.plot_decision_choice('blue',str(vis.pca_var)+\" lockout\",title, ax)\n",
    "\n",
    "# vis.pca_flag = True\n",
    "# vis.pca_var = 0.99\n",
    "# ax=plt.subplot(2,3,2)\n",
    "# vis.plot_decision_choice('blue','xxpcs_0.99Variance',title, ax)\n",
    "\n",
    "\n",
    "# vis.pca_flag = True\n",
    "# vis.pca_var = 0.95\n",
    "# ax=plt.subplot(2,3,3)\n",
    "# vis.plot_decision_choice('red','8pcs_0.95Variance',title, ax)\n",
    "\n",
    "# vis.pca_flag = True\n",
    "# vis.pca_var = 0.90\n",
    "# ax=plt.subplot(2,3,4)\n",
    "# vis.plot_decision_choice('green','4pcs_0.90Variance',title, ax)\n",
    "\n",
    "\n",
    "# vis.pca_var = 0.50\n",
    "# ax=plt.subplot(2,3,5)\n",
    "# vis.plot_decision_choice('magenta','2pcs_0.50Variance',title, ax)\n",
    "\n",
    "\n",
    "# fname = vis.main_dir+'/'+animal_id+'/SVM_scores_'+animal_id+'_lockout_'+str(session)+'.npy'\n",
    "# vis.load_data(fname)\n",
    "# vis.plot_decision_choice('blue','lockout-denoised')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 16384)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "file = open('/media/cat/4TBSSD/yuki/IA1/tif_files/IA1pm_Feb3_30Hz/IA1pm_Feb3_30Hz_code_04_lockout_10sec_trial_ROItimeCourses_10sec_pca.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "pca = pk.load(file)\n",
    "\n",
    "print (pca.components_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 601, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "data_stm = np.load('/media/cat/4TBSSD/yuki/IA1/tif_files/IA1pm_Feb3_30Hz/IA1pm_Feb3_30Hz_10.0sec_butterworth_globalAverage_0.1hz_6.0hz_04code_stm.npy')\n",
    "print (data_stm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nComp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f869b1fe5d18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m Xnew = np.dot(pca.transform(X)[:,:nComp],\n\u001b[0m\u001b[1;32m      8\u001b[0m              pca.components_[:nComp,:])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nComp' is not defined"
     ]
    }
   ],
   "source": [
    "# \n",
    "X = data_stm.reshape(data_stm.shape[0]*data_stm.shape[1],\n",
    "                     data_stm.shape[2]*data_stm.shape[3])\n",
    "\n",
    "mu= np.mean(X, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComp = 9\n",
    "Xnew = np.dot(pca.transform(X)[:,:nComp],\n",
    "             pca.components_[:nComp,:])\n",
    "\n",
    "Xnew+=mu\n",
    "\n",
    "data_stm_denoised_reshaped = Xnew.reshape(data_stm.shape[0], data_stm.shape[1],\n",
    "                                          data_stm.shape[2], data_stm.shape[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 9, 601)\n"
     ]
    }
   ],
   "source": [
    "time_filters = pca.transform(X)[:,:nComp]\n",
    "pca_time_filters_only = time_filters.reshape(data_stm.shape[0], data_stm.shape[1],-1).transpose(0,2,1)\n",
    "print (pca_time_filters_only.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 601, 9)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 2, 601)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/4TBSSD/yuki/IA1/tif_files/IA1pm_Feb23_30Hz/IA1pm_Feb23_30Hz_code_04_trial_ROItimeCourses_10sec_pca_0.5.npy')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 35, 601)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Cat Data.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1r-NABuBJOITVlW03VvhsQivPNzmNp4yp\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "import random\n",
    "from scipy import stats\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import os\n",
    "\n",
    "\n",
    "jobid = os.getenv('SLURM_ARRAY_TASK_ID')\n",
    "name='IA1_lockout'\n",
    "\n",
    "data_leverpress = np.load('/media/cat/4TBSSD/yuki/temp/IA1/data_04.npy',allow_pickle=True)[0]\n",
    "data_random = np.load('/media/cat/4TBSSD/yuki/temp/IA1/data_04_random.npy',allow_pickle=True)[0]\n",
    "\n",
    "print (data_leverpress.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
      "<ipython-input-77-0c90c545bbff>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#classification of time (10-class)\n",
    "X_l=data_leverpress #  [n_trials, n_areas, n_time_steps]\n",
    "X_l=X_l.transpose((0,2,1))\n",
    "\n",
    "\n",
    "X_R_l= X_l.reshape(-1,X_l.shape[1]*X_l.shape[2])\n",
    "normal_X_l = preprocessing.normalize(X_R_l)\n",
    "n_X_l=normal_X_l.reshape(X_l.shape[0],X_l.shape[1],X_l.shape[2])\n",
    "X_l=n_X_l\n",
    "\n",
    "X_l=X_l[:,:300,:]\n",
    "X_k=X_l[:,0:30,:]\n",
    "X_k=X_k.reshape(X_k.shape[0],X_k.shape[1]*X_k.shape[2])\n",
    "for i in range(30,271,30):\n",
    "    X_t = X_l[:,i:i+30,:]\n",
    "    X_t=X_t.reshape(X_t.shape[0],X_t.shape[1]*X_t.shape[2])\n",
    "    X_k=np.concatenate((X_k,X_t),axis=0)\n",
    "\n",
    "y_ct=np.zeros(data_leverpress.shape[0])\n",
    "for i in range(1,10):\n",
    "    Z_ct=i*np.ones(data_leverpress.shape[0])\n",
    "    y_ct=np.concatenate((y_ct,Z_ct))\n",
    "\n",
    "# \n",
    "X_tSVM=X_k # 10s * 30frams\n",
    "y_tSVM=y_ct # labels\n",
    "\n",
    "#10-fold confusion matrix\n",
    "clf = svm.SVC() # Non-linear classifier\n",
    "ten_svm=[]\n",
    "conf_matrix_ten=[]\n",
    "kf = KFold(n_splits=10,random_state=None, shuffle=True) \n",
    "for train_index, test_index in kf.split(X_tSVM):\n",
    "    X_train_k, X_test_k = X_tSVM[train_index], X_tSVM[test_index] \n",
    "    y_train_k, y_test_k = y_tSVM[train_index], y_tSVM[test_index]\n",
    "    clf.fit(X_train_k, y_train_k)\n",
    "    score=clf.score(X_test_k, y_test_k)\n",
    "    y_predicted=clf.predict(X_test_k)\n",
    "    cm=confusion_matrix(y_test_k,y_predicted)\n",
    "    confusion_m=cm.T # make each row be the prediction\n",
    "    conf_matrix_norm = confusion_m.astype('float') / confusion_m.sum(axis=1)[:,np.newaxis] #calculate the precision\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    ten_svm.append(score)\n",
    "    conf_matrix_ten.append(conf_matrix_norm)\n",
    "np.save('conf_10_'+str(name)+'_'+str(jobid)+'.npy',conf_matrix_ten)\n",
    "sc=np.mean(conf_matrix_ten,axis=0) # mean of confusion matrix\n",
    "np.save('conf_avg_'+str(name)+'_'+str(jobid)+'.npy',sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-40a74f02ec5b>:57: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  t=(a-(1/10))/s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Next is the plotting.\n",
    "\n",
    "confusion_10=sc.diagonal()\n",
    "np.save('diagonal_'+str(name)+'_'+str(jobid)+'.npy',confusion_10)\n",
    "confusion_s=conf_matrix_ten\n",
    "\n",
    "confusion_d=[]\n",
    "for i in range(0,10):\n",
    "    confusion_d.append(confusion_s[i].diagonal())\n",
    "x_std=np.std(confusion_d,axis=0)/(10**0.5)\n",
    "x_s=x_std\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(21.5,20), dpi=64, facecolor='white')\n",
    "plt.xticks(fontsize=60)\n",
    "plt.yticks(fontsize=60)\n",
    "axes = plt.subplot(111)\n",
    "axes.tick_params(axis ='both', which ='both', length = 10,width=4,pad=20)\n",
    "\n",
    "bwith = 3 \n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_linewidth(bwith)\n",
    "ax.spines['left'].set_linewidth(bwith)\n",
    "ax.spines['top'].set_linewidth(bwith)\n",
    "ax.spines['right'].set_linewidth(bwith)\n",
    "l=15\n",
    "\n",
    "Y=np.linspace(0,1,12)\n",
    "X=np.ones(Y.size)\n",
    "x=np.arange(-9.5,0.5,1)\n",
    "plt.text(0.1, 0.2, 'Lever Pull', fontdict={'size': 70}, rotation=90)\n",
    "plt.text(-12.95, 1/10, 'Chance', fontdict={'size': 60}, rotation=0)\n",
    "plt.axhline(y=1/10, color='black', linestyle='--',linewidth=6)\n",
    "\n",
    "plt.errorbar(x,confusion_10,x_s,c='red',alpha=0.6,marker='s', mec='red', ms=3, mew=3,label='SVM',linewidth=l,elinewidth=12)\n",
    "\n",
    "plt.plot( (0+0)*X, Y, color='black',ls='--',linewidth=6)\n",
    "\n",
    "plt.title('Decoding decision time',fontdict={'size': 110},pad=60)\n",
    "plt.xlabel('Time before Lever Pull (s)',fontdict={'size': 110},labelpad=60)\n",
    "plt.ylabel('Decoding Accuracy',fontdict={'size': 110},labelpad=180)\n",
    "plt.ylim(ymax = 1.09)\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "plt.xlim(xmin = -10.5)\n",
    "plt.xlim(xmax = 0.9)\n",
    "\n",
    "for i in range(0,10):\n",
    "    a=confusion_10[i]\n",
    "    b=x_s[i]\n",
    "    n=10\n",
    "    s=b\n",
    "    t=(a-(1/10))/s\n",
    "    df=n-1\n",
    "    p = (1 - stats.t.cdf(t,df=df))\n",
    "    \n",
    "    if (p > 0.01 and p <= 0.05):\n",
    "        plt.plot(-9.5+i, confusion_10[i]+x_std[i]+0.06, '*',c='black', ms=20)\n",
    "        \n",
    "    if (p > 0.00001 and p <= 0.01):\n",
    "        plt.plot(-9.5+i,confusion_10[i]+x_std[i]+0.06,'*',c='black', ms=20)\n",
    "        plt.plot(-9.5+i,confusion_10[i]+x_std[i]+0.08,'*',c='black', ms=20)\n",
    "    if  p <= 0.00001:\n",
    "        plt.plot(-9.5+i,confusion_10[i]+x_std[i]+0.06,'*',c='black', ms=20)\n",
    "        plt.plot(-9.5+i,confusion_10[i]+x_std[i]+0.08,'*',c='black', ms=20)\n",
    "        plt.plot(-9.5+i,confusion_10[i]+x_std[i]+0.10,'*',c='black', ms=20)\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('./time_svm_'+str(jobid)+'.jpg')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 16384)\n"
     ]
    }
   ],
   "source": [
    "data= np.load('/media/cat/4TBSSD/yuki/IJ2/tif_files/IJ2pm_Mar3_30Hz/IJ2pm_Mar3_30Hz_code_04_lockout_10sec_random_ROItimeCourses_10sec_pca_0.95_spatial.npy')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 20, 1801)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/4TBSSD/yuki/IJ2/tif_files/IJ2pm_Mar1_30Hz/IJ2pm_Mar1_30Hz_code_04_lockout_10sec_random_ROItimeCourses_30sec_pca_0.999.npy')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
